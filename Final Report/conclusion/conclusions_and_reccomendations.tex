\chapter{Conclusions and Reccomendations}

\section{Experimental Conclusions}

In summary, DDSP has demonstrated itself as a very powerful tool for learning and synthesizing vocal features of the human voice. The experiments shown in this paper how it was able to learn how to accurately infer pitch and phoentic information about the human voice. This is significant as it appears to overcome the problems of the original MFCC DDSP model\cite{SingingDDSP} which was unable to learn the underlying phonemes of speech and produce coherent speech.

It could be said that despite the failure of the models in this paper to trasnfer timbre and loudness, the interpretable pitch feature of DDSP has been successfilly learned and demonstrated in a variety of sitations that demonstate genarability outside of the training dataset.

Further applicability previously not explored in the DDSP paper, such as vocal source separation has also been demonstrated. This is due to the fact that the model had successfully learned to infer the vocal source from the audio signal.

DDSP will have a future in synthesing realistic music vocals and could be used for further accademic or artistic purposes. Its interpretable and modular nature will allow it to be integrated into other applications.

\section{Reccomendations}

There are however many areas of improvement that can be made to the model. Firstly, accurate loudness estimation must be implemented, this is a feature of the original DDSP model\cite{OriginalDDSP} that appears to not be working with the variation applied to singing. This may require an extensive review of the decoder to ensure it is programmed correctly. If it is found to be working, then the loss function may have to be altered to bias the model towards the amplitude latent vector.

The timbral quality although good could be better, this could be improved by increasing the number of internal parameters and increasing the resolution of spectrogtrams used. This change would increase model size significantly requiring its parellizaiton and training on multiple GPUs.

Future works could focus on implementing DDSP with other technologies and machine learning models. A DDSP based model could be combined with an attention based symbolic transformer model, natural langauge model such as GPT-3\cite{GPT3} and text to speech systems to provide an end to end program that could generate a whole song. The symbolic transformer model could generate long term features such as F0 over time (in the form of midi notation) as well as other long term musical features and characteristics. The natural language model could then be tasked with generating lyrics to match the timings of the generated music. These could be synthesized using text to speech and F0 or other characteristics modified by DDSP.