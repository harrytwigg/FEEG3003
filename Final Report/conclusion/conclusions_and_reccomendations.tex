\chapter{Conclusions and Reccomendations}

\section{Experimental Conclusions}

In summary, DDSP has demonstrated itself as a powerful tool for learning and synthesising the vocal features of the human voice. The experiments shown in this thesis demonstrated how it could accurately infer pitch and phonetic information about the human voice. This result is significant as it appears to overcome some of the problems of the original MFCC DDSP model\cite{SingingDDSP} which was unable to learn the underlying phonemes of speech and produce coherent speech. Furthermore, generality outside of the training dataset was demonstrated in many situations.

It could be said that despite the failure of the models in this thesis to transfer timbre and loudness, the interpretable pitch feature of DDSP has been successfully learned and demonstrated.

Further applicability previously not explored in the DDSP paper, such as vocal source separation, has also been demonstrated. The model had successfully learned to infer only the vocal source from the audio signal.

DDSP is already one of the best deep learning architectures for synthesising realistic music vocals. It will likely be used for future academic or artistic purposes. Its interpretable and modular nature will allow it to be integrated into other applications. Further research into the alternatives, some of which were presented in this thesis, e.g. \nameref{sec:jukebox} is, however, required to evaluate if DDSP is the best.

\section{Reccomendations}

However, there are many areas of improvement that can be made to the model. Firstly, accurate loudness estimation must be implemented. Loudness control is a feature of the original DDSP model\cite{OriginalDDSP} that appears not to be working in the model used in this paper. Therefore, a solution may require an extensive decoder review. If it is found to be working as desired, the loss function may have to be altered to bias the model towards the amplitude latent vector.

The timbral quality, although good, could be better. Timbral quality could be improved by increasing the number of internal parameters and increasing the resolution of spectrograms used. However, this change would increase the model size significantly, requiring its parallelisation and training on multiple GPUs. Furthermore, timbral transfer like that demonstrated in the original DDSP paper did not work for the models trialled in this thesis. Future work could pertain to addressing this discrepancy.

Future works could implement DDSP with other technologies and machine learning models. For example, a DDSP based model could be combined with an attention-based symbolic transformer model, a natural language model such as GPT-3\cite{GPT3} and text to speech systems to provide an end to end program that could generate a whole song. The symbolic transformer model could generate long-term features such as F0 over time (in the form of midi notation) and other long-term musical features and characteristics. The natural language model could then be tasked with generating lyrics to match the timings of the generated music. These could be synthesised using text to speech and F0 or other characteristics modified by DDSP.

As suggested by previous work\cite{SingingDDSP}, native phonetic conditioning could be implemented into DDSP, enabling modification of the underlying words and phonetics as latents similarly to F0 or loudness. Implementing this would enable further expressibility in the DDSP architecture and avoid the need for any other text to speech model.