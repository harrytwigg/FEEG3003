\chapter{Conclusions and Reccomendations}

\section{Experimental Conclusions}

In summary, DDSP has demonstrated itself as a very powerful tool for learning and synthesizing vocal features of the human voice. The experiments shown in this paper how it was able to learn how to accurately infer pitch and phoentic information about the human voice. This is significant as it appears to overcome the problems of the original MFCC DDSP model\cite{SingingDDSP} which was unable to learn the underlying phonemes of speech and produce coherent speech.

\section{Reccomendations}

There are however many areas of improvement that can be made to the model. Firstly, accurate ampplitude estimation must be implemented, this is a feature of the original DDSP model\cite{OriginalDDSP} that appears to not be working with the variation applied to singing. This may require an extensive review of the decoder to ensure it is programmed correctly. If it is found to be working, then the loss function may have to be altered to bias the model towards the amplitude latent vector.

The timbral quality although good could be better, this could be improved by increasing the number of internal parameters and increasing the resolution of spectrogtrams used. This change would increase model size significantly requiring its parellizaiton and training on multiple GPUs.

One area could address the difficulty of the model in pronouncing words. A solution could be via phoentic conditioning, as outlined in previous work\cite{SingingDDSP}. This could take the form of an encoder that takes in written phoentic information or word pronunciation, it could then potentially use this information to output pitch, amplitude and timbre envelopes. These could then be passed into a trained decoder, which could in turn synthesize actual vocalised words. Although phoentic condition may not be the best solution, one must be found so that DDSP vocal sound synthesis can be further built upon.

Alternatively an encoder could be devised that converts symbolic representaition into DDSP. Seeing how long term structure has already been demonstrated in symbolic data\cite{Attention}, this higher level structural information could somehow be encoded into DDSP. In this way musical vocals (or other musical sounds such as instruments) could be genreated that contains accurate local harmonoic and noise information that sounds accurate to a listener. Such a model would bridge the gap between higher and lower level representations.