\section{Methodology}

A critical review of existing literature on music sound synthesis (with a focus on vocal sound synthesis) was conducted. A standardised processs was developed to evaluate the quality of existing methods. This was necessary as it would be difficult to compare the results of different methods. Each of the approaches evaluated used different levels of abstraction and resolution of musical and audio data. Additionally they have different trade-offs in terms of accuracy and computational efficiency.

The standardised process is based on good principles in machine learning, along with accademic best practices. The technical requirements are as follows:

\subsection{Technical Requirements}

\begin{itemize}
    \item Overly time consuming methods should be penalised due to the limited time for the project.These can come in many forms eg. excessive training and computation time or extremely large datasets requirements, excessive hyperparameter tuning, or overly large networks
    \item Use of teacher forcing or operator involvement in any methods, as this will lead to biases in the model outputs and limits the scalability and ease of use of any derived models. Manually labelled data shall also be penalised similarly.
    \item Poor tonal quality in the output, eg. it is noticeable that the model was generated digitally as opposed to recorded. This could be caused by:
          \begin{itemize}
              \item Spectral leakage due to inaccuracies in fourier representations
              \item Poor oscillatory output representation that does not sound natural
          \end{itemize}
    \item To analyse the tonal quality a statistical method of loss must be defined and used
    \item Modular systems shall be evaluated positively, due to the fact that their individual elements can be built on separately, and the whole system acts less like a 'Black Box'.
    \item Any discarded information eg phase that has been discarded during encoding (eg. phase) that could be presented to the network shall also be penalised. It is hypothesized that this information could be used to improve the quality of the output.
    \item Model architectures that are specific to music and audio signal processing are preferred, as opposed to more general machine learning problems. It was believed that directing the model towards specific musical features (such as harmonics and pitch) would be beneficial, rather than generalising to the entire audio signal.
\end{itemize}

\subsection{Academic Requirements}

Well citided papers or those that are in scientific journals were looked upon favourablly, showing that other people have found the work to be valuable and more importantly, credible. It was also desired that any researched papers have open sourced code, and that the code is available for use. Without this the model cannot be easily built off of without building the codebase from the ground up, which would take considerable time. Older methods that have not been built further were evaluated negatively, as this suggests that experts in the field have judged the work to be of no further benefit and hence obscelete.

\subsection{DDSP Training}

After the intial research, DDSP was investigated further as it was the most promising method due to several factors discussed later in \nameref{section:DDSP}.

Two different datasets were created, one male voice artist and one female voice artist, to see how the DDSP architecture would handle male and female voices differently.

For each artist two different albums were picked of similar musical style to ensure consistency of vocal style across the entire dataset.

The albums were picked so that it only had one voice on the vocal track to avoid any problems of the model mixing voices, any songs with cover artists or different singers to the main voice were removed. Each dataset was made to be approximately of similar length, this was done to ensure similar levels of generalisation between the two models.

Each dataset was processed through a pre-trained model called Spleter\cite{Spleeter}. This pretrained model separated the vocal track from the instrumentals for each song in the album.

The albums were picked so that it only had one voice on the vocal track to avoid any problems of the model mixing voices.

Each dataset was then trained using the DDSP library\cite{DDSPPip} and code adapted from a variation of DDSP designed for singing\cite{SingingDDSP}. Model hyperparameters were kept the same as in the Singing DDSP paper\cite{SingingDDSP} as the researchers had demonstrated thorough testing of which hyperparameters were the best.

\subsection{DDSP Inferencing}

Following training of both models, the models underwent several inferencing tests:

\begin{enumerate}
    \item The models were inferenced on the same dataset that was used for training to test the models acruaccy at predicting the training dataset.
    \item A pitch transposition was attempted. Vocal samples from the original datasret were transposed, up an and down an octave to see how the model would perform on unseen vocal ranges.
    \item A mono-pitch inference test was condicted to determine the models ability at producing a single pitch
    \item A log-linear loudness inference was attempted to gauge the models ability at modifying the loudness of the vocal samples.
    \item For the best dataset, 2 unseen vocal tracks of different artists were put into the model to see how the model would perform on unseen vocal samples of different artists in order top gauge the models generalisation ability to other artists and unseen voice types.
\end{enumerate}

Finally, in light of the experimental results and other accademic research, recommendations for future work in the field as a whole are made.