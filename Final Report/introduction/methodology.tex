\section{Methodology}

A standardised process was developed to evaluate the quality of existing methods, and a critical review of existing literature on music sound synthesis (focusing on vocal sound synthesis) was conducted. This standardised process was necessary as it would be difficult to compare the results of different methods. Each of the approaches evaluated used different levels of abstraction and resolution of musical and audio data. Additionally, they have different trade-offs in terms of accuracy and computational efficiency.

The standardised process is based on good machine learning principles and academic best practices. The technical requirements are as follows:

\subsection{Technical Requirements}

\begin{itemize}
    \item Overly time-consuming methods should be penalised due to the limited time for the project. These can come in many forms, e.g. excessive training and computation time or extensive datasets requirements, excessive hyperparameter tuning, or overly large networks
    \item Use of teacher forcing or operator involvement in any methods. Teacher forcing leads to biases in the model outputs and limits the scalability and ease of using any derived models. Manually labelled data shall also be penalised similarly.
    \item Poor tonal quality in the output, e.g. it is noticeable that the model was generated digitally instead of recorded. Poor tonal quality could be caused by:
          \begin{itemize}
              \item Spectral leakage due to inaccuracies in Fourier representations
              \item Poor oscillatory output representation that sounds synthetic
          \end{itemize}
    \item To analyse the tonal quality, a statistical method of loss must be defined and used
    \item Modular systems shall be evaluated positively because their elements can be built on separately, and the whole system acts less like a 'Black Box'.
    \item Any discarded information, e.g. phase that has been discarded during encoding (e.g. phase) that could be presented to the network shall also be penalised. It is hypothesised that this information could be used to improve the quality of the output.
    \item Model architectures specific to music and audio signal processing were preferred instead of more general ones. Furthermore, it was believed that directing the model towards specific musical features (such as harmonics and pitch) would be beneficial, rather than generalising to the entire audio signal.
\end{itemize}

\subsection{Academic Requirements}

Well cited papers or those in scientific journals were looked upon favourably, showing that other people have found the work valuable and, more importantly, credible. It was also desired that any researched papers have open-sourced code and that the code is available for use. Without this, the model cannot be quickly built without building the codebase from the ground up, which would take considerable time. Older methods that have not been built further were evaluated negatively, as this suggests that experts in the field have judged the work to be of no further benefit and hence obsolete.

\subsection{DDSP Training}

After the initial research, DDSP, a modular approach, was picked; it enabled modification of evaluated sound qualities called latents (pitch, loudness) and a series of differentiable versions of traditional signal processing techniques. DDSP was the most promising model architecture due to several factors discussed in \nameref{section:DDSP}.

Two different datasets were created, one male voice artist and one female voice artist, to see how the DDSP architecture would handle male and female voices differently.

For each artist, two different albums were picked of similar musical styles to ensure consistency of vocal style across the entire dataset; this was done to try and encourage the model to learn a specific timbre of voice.

The albums were picked so that they only had one voice on the vocal track to avoid any problems of the model mixing voices, any songs with cover artists or different singers to the leading voice were removed.

Each dataset was processed through a pre-trained model called Spleter\cite{Spleeter}. This pre-trained model separated the vocal track from the instrumentals for each song in the album. Consequently, large datasets could be easily created featuring a singular vocal track and enabling datasets 10x the size of the paper this work was based on\cite{SingingDDSP}.

Each dataset was then trained using the DDSP library\cite{DDSPPip} and code adapted from a variation of DDSP designed for singing\cite{SingingDDSP}. Model hyperparameters were kept the same as in the Singing DDSP paper\cite{SingingDDSP} as the researchers had demonstrated thorough testing of which hyperparameters were the best. 200,000 epochs were used for each dataset; this was deemed sufficient for this project whilst keeping the training time manageable.

\subsection{DDSP Inferencing}

Following the training of both models, the models underwent several inferencing tests designed to evaluate the models' encoding of the latent characteristics.

\begin{enumerate}
    \item The models were inferred on the same dataset used for training to test the model's accuracy in predicting frames from the training dataset.
    \item A pitch transposition was attempted. Vocal samples from the original dataset were transposed up and down an octave to see how the model would perform on unseen vocal ranges.
    \item A mono-pitch inference test was conducted to determine the model's ability at producing a single pitch.
    \item A log-linear loudness inference was attempted to gauge the model's ability to modify the loudness of the vocal samples.
    \item For the best model timbral transfer tests were conducted, the best was determined by performance in previous inferencing tests. Tests were conducted on an unseen male and female voice to determine performance with different voice types.
\end{enumerate}

Finally, in light of the experimental results and other academic research, recommendations for future work in the field are made.