\section{Methodology}

\subsection{Technical Requirements}

A set of technical reqauirements were used to evaluate DDSP and other model architectures shown in this thesis. These are important for evaluating the utility of DDSP and other model architectures to achieve programmatic and rapid vocal synthesis goals, as outlined in the \nameref{ch:introduction}.

\begin{itemize}
    \item Use of teacher forcing or operator involvement in any methods. Teacher forcing leads to biases in the model outputs and limits the scalability and ease of using any derived models. Manually labelled data shall also be penalised similarly.
    \item Poor tonal quality in the output, e.g. it is noticeable that the model was generated digitally instead of recorded. Poor tonal quality could be caused by:
    \item Modular systems shall be evaluated positively because their elements can be built on separately, and the whole system acts less like a 'Black Box'.
    \item Any discarded information, e.g. phase that has been discarded during encoding (e.g. phase) that could be presented to the network shall also be penalised. It is hypothesised that this information could be used to improve the quality of the output.
    \item Model architectures specific to music and audio signal processing were preferred instead of more general ones. Furthermore, it was believed that directing the model towards specific musical features (such as harmonics and pitch) would be beneficial, rather than generalising to the entire audio signal.
\end{itemize}

\subsection{Academic Requirements}

Well cited papers or those in scientific journals were looked upon favourably, showing that other people have found the work valuable and, more importantly, credible. It was also desired that any researched papers have open-sourced code and that the code is available for use. Without this, the model cannot be quickly built without building the codebase from the ground up, which would take considerable time. Older methods that have not been built further were evaluated negatively, as this suggests that experts in the field have judged the work to be of no further benefit and hence obsolete.

\subsection{DDSP Training}

DDSP is fundamentaly a modular and extendable deep learning archtecture, enabling the timewise modification of evaluated sound qualities called latents (pitch andloudness). Additionally, it features a series of differentiable versions of traditional signal processing techniques. After background research DDSP appeared the most promising model architecture due to several factors discussed in \nameref{section:DDSP}.

Two different datasets were created, one male voice artist and one female voice artist, to see how the DDSP architecture would handle male and female voices differently.

For each artist, two different albums were picked of similar musical styles to ensure consistency of vocal style across the entire dataset; this was done to try and encourage the model to learn a specific timbre of voice.

The albums were picked so that they only had one voice on the vocal track to avoid any problems of the model mixing voices, any songs with cover artists or different singers to the leading voice were removed.

Each dataset was processed through a pre-trained model called Spleter\cite{Spleeter}. This pre-trained model separated the vocal track from the instrumentals for each song in the album. Consequently, large datasets could be easily created featuring a singular vocal track and enabling datasets 10x the size of the paper this work was based on\cite{SingingDDSP}.

Two models were then trained using the DDSP library\cite{DDSPPip}, one for for each respective dataset. The code for these were derived from the DDSP Library and a variation of DDSP designed for singing\cite{SingingDDSP}. Model hyperparameters were kept the same as in the Singing DDSP paper\cite{SingingDDSP} as the researchers had demonstrated thorough testing of which hyperparameters were the best. 200,000 epochs were used for each dataset; this was deemed sufficient for this project whilst keeping the training time manageable.

\subsection{DDSP Inferencing}

Following the training of both models, the models underwent several inferencing tests designed to evaluate the models's encoding of the latent characteristics, and to evaluate the flexability of the DDSP architecture in a variety of different situations.

\begin{enumerate}
    \item The models were inferred on the same dataset used for training to test the model's accuracy in predicting frames from the training dataset.
    \item A pitch transposition was attempted. Vocal samples from the original dataset were transposed up and down an octave to see how the model would perform on unseen vocal ranges.
    \item A mono-pitch inference test was conducted to determine the model's ability at producing a single pitch.
    \item A log-linear loudness inference was attempted to gauge the model's ability to modify the loudness of the vocal samples.
    \item For the best model timbral transfer tests were conducted, the best was determined by performance in previous inferencing tests. Tests were conducted on an unseen male and female voice to determine performance with different voice types. The test frames were separated similarly to the test datasets using Spleeter. In the tests, all latents remained unmodified to isolate the effect of changing the vocal artist.
    \item Again on the best model an inference test on a track with instrumentals was conducted to observe the models performance on unseen instrumentals within a track. Again, all latents remained unmodified to isoalate any effect of the instrumentals. The track was the female voice artist from the timbral trasnfer test. The same timestamped frame was used so the output between the inferred frames for both tracks could be compared.
\end{enumerate}

Finally, in light of the experimental results and other academic research, recommendations for future work in the field are made.